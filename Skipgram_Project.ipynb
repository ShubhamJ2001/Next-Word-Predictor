{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a9a64af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "\n",
    "class word2vec(object):\n",
    "    def __init__(self):\n",
    "        self.N = 10\n",
    "        self.X_train = []\n",
    "        self.y_train = []\n",
    "        self.window_size = 3\n",
    "        self.alpha = 0.001\n",
    "        self.words = []\n",
    "        self.word_index = {}\n",
    "\n",
    "    def initialize(self, V, data):\n",
    "        self.V = V\n",
    "        self.W = np.random.uniform(-0.8, 0.8, (self.V, self.N))  # vector\n",
    "        self.W1 = np.random.uniform(-0.8, 0.8, (self.N, self.V))  # weight\n",
    "        self.words = data\n",
    "        for i in range(len(data)):\n",
    "            self.word_index[data[i]] = i\n",
    "\n",
    "    def feed_forward(self, X):\n",
    "        self.h = np.dot(self.W.T, X).reshape(self.N, 1)\n",
    "        self.u = np.dot(self.W1.T, self.h)\n",
    "        # print(self.u)\n",
    "        self.y = softmax(self.u)\n",
    "        return self.y\n",
    "\n",
    "    def backpropagate(self, x, t):\n",
    "        e = self.y - np.asarray(t).reshape(self.V, 1)\n",
    "        # e.shape is V x 1\n",
    "        dLdW1 = np.dot(self.h, e.T)\n",
    "        X = np.array(x).reshape(self.V, 1)\n",
    "        dLdW = np.dot(X, np.dot(self.W1, e).T)\n",
    "        self.W1 = self.W1 - self.alpha * dLdW1  # adjusted weights and vectors\n",
    "        self.W = self.W - self.alpha * dLdW\n",
    "\n",
    "    def train(self, epochs):\n",
    "        for x in range(1, epochs):\n",
    "            self.loss = 0\n",
    "            for j in range(len(self.X_train)):\n",
    "                self.feed_forward(self.X_train[j])\n",
    "                self.backpropagate(self.X_train[j], self.y_train[j])\n",
    "                C = 0\n",
    "                for m in range(self.V):\n",
    "                    if (self.y_train[j][m]):\n",
    "                        self.loss += -1 * self.u[m][0]\n",
    "                        C += 1\n",
    "                self.loss += C * np.log(np.sum(np.exp(self.u)))\n",
    "            print(x, \"epoch loss = \", self.loss)\n",
    "            self.alpha *= 1 / ((1 + self.alpha * x))\n",
    "\n",
    "    def predict(self, word, number_of_predictions):\n",
    "        l = list(word.split(' '))\n",
    "        if len(l) < 1:\n",
    "            if word in self.words:\n",
    "                index = self.word_index[word]\n",
    "                X = [0 for i in range(self.V)]\n",
    "                X[index] = 1\n",
    "                prediction = self.feed_forward(X)\n",
    "                output = {}\n",
    "                for i in range(self.V):\n",
    "                    output[prediction[i][0]] = i\n",
    "\n",
    "                top_context_words = []\n",
    "                for k in sorted(output, reverse=True):\n",
    "                    top_context_words.append(self.words[output[k]])\n",
    "                    if len(top_context_words) >= number_of_predictions:\n",
    "                        break\n",
    "\n",
    "                return top_context_words\n",
    "            else:\n",
    "                print(\"Word not found in dictionary\")\n",
    "        else:\n",
    "            result = []\n",
    "            for x in l:\n",
    "                if x in self.words:\n",
    "                    index = self.word_index[x]\n",
    "                    X = [0 for i in range(self.V)]\n",
    "                    X[index] = 1\n",
    "                    prediction = self.feed_forward(X)\n",
    "                    output = {}\n",
    "                    for i in range(self.V):\n",
    "                        output[prediction[i][0]] = i\n",
    "\n",
    "                    top_context_words = []\n",
    "                    for k in sorted(output, reverse=True):\n",
    "                        top_context_words.append(self.words[output[k]])\n",
    "                        if len(top_context_words) >= number_of_predictions:\n",
    "                            break\n",
    "                    result.append(top_context_words)\n",
    "                else:\n",
    "                    print(\"{} not found in dictionary\".format(x))\n",
    "            return result\n",
    "\n",
    "\n",
    "\n",
    "def preprocessing(corpus):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    training_data = []\n",
    "    sentences = corpus.split(\".\")\n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = sentences[i].strip()\n",
    "        sentence = sentences[i].split()\n",
    "        x = [word.strip(string.punctuation) for word in sentence\n",
    "             if (word not in stop_words and word.isalpha() == True)]\n",
    "        x = [word.lower() for word in x]\n",
    "        training_data.append(x)\n",
    "    return training_data\n",
    "\n",
    "\n",
    "def prepare_data_for_training(sentences, w2v):\n",
    "    data = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            if word not in data:\n",
    "                data[word] = 1\n",
    "            else:\n",
    "                data[word] += 1\n",
    "    V = len(data)\n",
    "\n",
    "    data = sorted(list(data.keys()))\n",
    "    vocab = {}\n",
    "    for i in range(len(data)):\n",
    "        vocab[data[i]] = i\n",
    "    print(vocab)\n",
    "\n",
    "    # for i in range(len(words)):\n",
    "    for sentence in sentences:\n",
    "        for i in range(len(sentence)):\n",
    "            center_word = [0 for x in range(V)] #window chala rhe\n",
    "            center_word[vocab[sentence[i]]] = 1\n",
    "            context = [0 for x in range(V)]\n",
    "\n",
    "            for j in range(i - w2v.window_size, i + w2v.window_size):\n",
    "                if i != j and j >= 0 and j < len(sentence):\n",
    "                    context[vocab[sentence[j]]] += 1\n",
    "            w2v.X_train.append(center_word)\n",
    "            w2v.y_train.append(context)\n",
    "    w2v.initialize(V, data)\n",
    "\n",
    "    return w2v.X_train, w2v.y_train, vocab\n",
    "\n",
    "\n",
    "file = open(\"HarryPotter1.txt\", \"r\", encoding=\"utf8\")\n",
    "corpus = file.read()\n",
    "epochs = 15  # accuracy bad because data large so less number of epochs iterated\n",
    "# data cleaning also requires more work punctuation marks and gaps and also numbers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1922100a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0, 'able': 1, 'about': 2, 'across': 3, 'act': 4, 'acting': 5, 'admiring': 6, 'affect': 7, 'after': 8, 'afternoon': 9, 'age': 10, 'agree': 11, 'air': 12, 'albus': 13, 'all': 14, 'allowed': 15, 'almost': 16, 'also': 17, 'although': 18, 'always': 19, 'amount': 20, 'amuse': 21, 'and': 22, 'angrily': 23, 'angry': 24, 'another': 25, 'answer': 26, 'anxious': 27, 'anyone': 28, 'anything': 29, 'anywhere': 30, 'apart': 31, 'appeared': 32, 'approve': 33, 'arm': 34, 'armchair': 35, 'arms': 36, 'around': 37, 'arrived': 38, 'as': 39, 'ask': 40, 'asked': 41, 'asleep': 42, 'astonishing': 43, 'astounding': 44, 'astride': 45, 'at': 46, 'aunt': 47, 'away': 48, 'baby': 49, 'back': 50, 'backed': 51, 'bakery': 52, 'balls': 53, 'bear': 54, 'beard': 55, 'bed': 56, 'bedroom': 57, 'beefy': 58, 'behaving': 59, 'behind': 60, 'believe': 61, 'belt': 62, 'beneath': 63, 'bent': 64, 'beside': 65, 'best': 66, 'bet': 67, 'better': 68, 'big': 69, 'bike': 70, 'birds': 71, 'bit': 72, 'black': 73, 'blame': 74, 'blankets': 75, 'blew': 76, 'blinked': 77, 'blonde': 78, 'blue': 79, 'blushed': 80, 'bolt': 81, 'bonfire': 82, 'books': 83, 'boots': 84, 'boring': 85, 'bound': 86, 'bowed': 87, 'boy': 88, 'breeze': 89, 'brick': 90, 'bring': 91, 'bringing': 92, 'bristol': 93, 'broad': 94, 'broke': 95, 'broken': 96, 'buckled': 97, 'building': 98, 'bun': 99, 'bunch': 100, 'bundle': 101, 'burying': 102, 'bushy': 103, 'business': 104, 'busy': 105, 'but': 106, 'buy': 107, 'call': 108, 'called': 109, 'calls': 110, 'calmly': 111, 'came': 112, 'car': 113, 'care': 114, 'carefully': 115, 'careless': 116, 'carrying': 117, 'casually': 118, 'cat': 119, 'catch': 120, 'cats': 121, 'caught': 122, 'celebrate': 123, 'celebrating': 124, 'celebrations': 125, 'cereal': 126, 'certainly': 127, 'chair': 128, 'changed': 129, 'child': 130, 'choosing': 131, 'chortled': 132, 'chuckled': 133, 'cigarette': 134, 'cleared': 135, 'clicked': 136, 'climbing': 137, 'cloak': 138, 'cloaks': 139, 'close': 140, 'closed': 141, 'clothes': 142, 'cloudy': 143, 'clutching': 144, 'collecting': 145, 'come': 146, 'comforting': 147, 'common': 148, 'complete': 149, 'completely': 150, 'concentrate': 151, 'confusing': 152, 'corner': 153, 'could': 154, 'country': 155, 'couple': 156, 'course': 157, 'cousin': 158, 'craning': 159, 'crept': 160, 'cried': 161, 'crowd': 162, 'cups': 163, 'curiously': 164, 'dabbed': 165, 'dare': 166, 'dared': 167, 'dark': 168, 'darkness': 169, 'dashed': 170, 'daughter': 171, 'day': 172, 'dead': 173, 'dear': 174, 'decided': 175, 'dedalus': 176, 'determined': 177, 'dialing': 178, 'different': 179, 'diggle': 180, 'dinner': 181, 'direction': 182, 'director': 183, 'disappeared': 184, 'discover': 185, 'distinctly': 186, 'disturb': 187, 'dog': 188, 'dolphins': 189, 'done': 190, 'door': 191, 'doughnut': 192, 'downpour': 193, 'downright': 194, 'dozen': 195, 'drawn': 196, 'dressed': 197, 'drifting': 198, 'drills': 199, 'drive': 200, 'driven': 201, 'driveway': 202, 'drop': 203, 'drops': 204, 'drove': 205, 'drummed': 206, 'dudley': 207, 'dumbledore': 208, 'dundee': 209, 'dursley': 210, 'dursleys': 211, 'early': 212, 'earmuffs': 213, 'edge': 214, 'eleven': 215, 'else': 216, 'emerald': 217, 'end': 218, 'engine': 219, 'enough': 220, 'enraged': 221, 'even': 222, 'evening': 223, 'ever': 224, 'every': 225, 'everyone': 226, 'everything': 227, 'everywhere': 228, 'exactly': 229, 'examined': 230, 'except': 231, 'excitedly': 232, 'expect': 233, 'experts': 234, 'explain': 235, 'eyed': 236, 'eyes': 237, 'face': 238, 'faltered': 239, 'family': 240, 'famous': 241, 'far': 242, 'fashion': 243, 'fast': 244, 'fear': 245, 'feasts': 246, 'feet': 247, 'fell': 248, 'finally': 249, 'find': 250, 'fine': 251, 'finer': 252, 'fingers': 253, 'finished': 254, 'firm': 255, 'firmly': 256, 'first': 257, 'five': 258, 'fixed': 259, 'flatter': 260, 'flicked': 261, 'flickered': 262, 'flocks': 263, 'flooded': 264, 'floor': 265, 'flutter': 266, 'flying': 267, 'fond': 268, 'for': 269, 'forehead': 270, 'forever': 271, 'forgotten': 272, 'forward': 273, 'found': 274, 'four': 275, 'frightened': 276, 'front': 277, 'frozen': 278, 'full': 279, 'funny': 280, 'future': 281, 'g': 282, 'garden': 283, 'gasped': 284, 'gave': 285, 'gazed': 286, 'gently': 287, 'get': 288, 'gets': 289, 'getting': 290, 'getups': 291, 'gingerly': 292, 'give': 293, 'glance': 294, 'glasses': 295, 'glowed': 296, 'glumly': 297, 'go': 298, 'going': 299, 'golden': 300, 'gone': 301, 'good': 302, 'gossiped': 303, 'got': 304, 'gray': 305, 'great': 306, 'greatest': 307, 'grew': 308, 'grin': 309, 'grip': 310, 'ground': 311, 'group': 312, 'growing': 313, 'grunnings': 314, 'hagrid': 315, 'hair': 316, 'half': 317, 'hand': 318, 'handkerchief': 319, 'hands': 320, 'handy': 321, 'happen': 322, 'happening': 323, 'happily': 324, 'happy': 325, 'hard': 326, 'harder': 327, 'hardly': 328, 'harold': 329, 'harry': 330, 'harvey': 331, 'he': 332, 'head': 333, 'heads': 334, 'heard': 335, 'heart': 336, 'heaven': 337, 'heavily': 338, 'hedges': 339, 'heel': 340, 'held': 341, 'help': 342, 'her': 343, 'hid': 344, 'hiding': 345, 'high': 346, 'his': 347, 'hissed': 348, 'hold': 349, 'holding': 350, 'hollow': 351, 'home': 352, 'hoped': 353, 'hoping': 354, 'horribly': 355, 'house': 356, 'how': 357, 'howl': 358, 'huddle': 359, 'huge': 360, 'hugged': 361, 'hummed': 362, 'hundreds': 363, 'hunt': 364, 'hurried': 365, 'husband': 366, 'hushed': 367, 'i': 368, 'if': 369, 'imagination': 370, 'imagining': 371, 'impatiently': 372, 'important': 373, 'improve': 374, 'in': 375, 'inky': 376, 'inside': 377, 'instead': 378, 'involved': 379, 'irritably': 380, 'it': 381, 'j': 382, 'jacket': 383, 'james': 384, 'jerked': 385, 'jim': 386, 'join': 387, 'judging': 388, 'jumping': 389, 'k': 390, 'keep': 391, 'keeping': 392, 'kent': 393, 'kicked': 394, 'kicking': 395, 'kill': 396, 'killed': 397, 'kind': 398, 'kiss': 399, 'knee': 400, 'knew': 401, 'knocked': 402, 'know': 403, 'knowing': 404, 'known': 405, 'knows': 406, 'lace': 407, 'laid': 408, 'lamp': 409, 'lamps': 410, 'landed': 411, 'large': 412, 'last': 413, 'late': 414, 'lay': 415, 'learned': 416, 'least': 417, 'leather': 418, 'left': 419, 'legend': 420, 'legs': 421, 'lemon': 422, 'lent': 423, 'less': 424, 'let': 425, 'letter': 426, 'light': 427, 'lighter': 428, 'lightning': 429, 'lights': 430, 'like': 431, 'liked': 432, 'lily': 433, 'lips': 434, 'little': 435, 'live': 436, 'lived': 437, 'living': 438, 'london': 439, 'long': 440, 'look': 441, 'looked': 442, 'looking': 443, 'lose': 444, 'lot': 445, 'lots': 446, 'louder': 447, 'loudly': 448, 'low': 449, 'lucky': 450, 'madam': 451, 'made': 452, 'make': 453, 'man': 454, 'map': 455, 'maps': 456, 'markings': 457, 'may': 458, 'maybe': 459, 'mcgonagall': 460, 'mcguffin': 461, 'mean': 462, 'meeting': 463, 'mention': 464, 'met': 465, 'middle': 466, 'midnight': 467, 'might': 468, 'milk': 469, 'mind': 470, 'minute': 471, 'minutes': 472, 'mirror': 473, 'mixed': 474, 'mixing': 475, 'moment': 476, 'mood': 477, 'morning': 478, 'most': 479, 'mother': 480, 'motorcycle': 481, 'move': 482, 'moved': 483, 'moving': 484, 'mr': 485, 'mrs': 486, 'much': 487, 'muffled': 488, 'muggle': 489, 'muggles': 490, 'mumbled': 491, 'murmured': 492, 'muscular': 493, 'must': 494, 'mustache': 495, 'mysterious': 496, 'name': 497, 'narrowed': 498, 'near': 499, 'nearest': 500, 'nearly': 501, 'neat': 502, 'neighbors': 503, 'neither': 504, 'nephew': 505, 'nerve': 506, 'nervously': 507, 'never': 508, 'new': 509, 'news': 510, 'newscaster': 511, 'next': 512, 'night': 513, 'nighttime': 514, 'ninth': 515, 'no': 516, 'noble': 517, 'nodded': 518, 'nodding': 519, 'none': 520, 'nonsense': 521, 'normal': 522, 'normally': 523, 'nose': 524, 'nothing': 525, 'notice': 526, 'noticed': 527, 'noticing': 528, 'number': 529, 'obviously': 530, 'odd': 531, 'oddly': 532, 'office': 533, 'old': 534, 'older': 535, 'on': 536, 'one': 537, 'onto': 538, 'opened': 539, 'opinion': 540, 'or': 541, 'orange': 542, 'order': 543, 'outer': 544, 'outside': 545, 'overhead': 546, 'owl': 547, 'owls': 548, 'page': 549, 'pair': 550, 'parking': 551, 'parties': 552, 'passed': 553, 'passersby': 554, 'past': 555, 'patted': 556, 'pattern': 557, 'patting': 558, 'pavement': 559, 'pecked': 560, 'peculiar': 561, 'peered': 562, 'people': 563, 'perfect': 564, 'perfectly': 565, 'perhaps': 566, 'person': 567, 'persuade': 568, 'petunia': 569, 'philosophers': 570, 'phoning': 571, 'picked': 572, 'piercing': 573, 'pinched': 574, 'pinpricks': 575, 'place': 576, 'plain': 577, 'planets': 578, 'pocket': 579, 'point': 580, 'pointed': 581, 'pointing': 582, 'pomfrey': 583, 'poor': 584, 'pop': 585, 'popped': 586, 'possible': 587, 'potter': 588, 'potters': 589, 'power': 590, 'powers': 591, 'precious': 592, 'pressed': 593, 'pretend': 594, 'pretended': 595, 'privet': 596, 'probably': 597, 'problems': 598, 'prodded': 599, 'professor': 600, 'promise': 601, 'promised': 602, 'proper': 603, 'proud': 604, 'pull': 605, 'pulled': 606, 'purple': 607, 'pursed': 608, 'put': 609, 'quickly': 610, 'quite': 611, 'quiver': 612, 'rain': 613, 'rather': 614, 'rattled': 615, 'reached': 616, 'read': 617, 'reading': 618, 'ready': 619, 'real': 620, 'realize': 621, 'realized': 622, 'really': 623, 'reason': 624, 'receiver': 625, 'related': 626, 'relieved': 627, 'repeated': 628, 'reply': 629, 'report': 630, 'reported': 631, 'right': 632, 'road': 633, 'roar': 634, 'rolled': 635, 'room': 636, 'rooted': 637, 'rose': 638, 'rowling': 639, 'ruffled': 640, 'rumbling': 641, 'rummaging': 642, 'rumor': 643, 'rumors': 644, 'said': 645, 'sat': 646, 'saw': 647, 'say': 648, 'saying': 649, 'scar': 650, 'scars': 651, 'scream': 652, 'screaming': 653, 'seconds': 654, 'secret': 655, 'secretary': 656, 'see': 657, 'seeing': 658, 'seem': 659, 'seemed': 660, 'seems': 661, 'seen': 662, 'seized': 663, 'sense': 664, 'sensible': 665, 'seriously': 666, 'set': 667, 'several': 668, 'shaggy': 669, 'shake': 670, 'shall': 671, 'shape': 672, 'shaped': 673, 'sharp': 674, 'sharply': 675, 'she': 676, 'shocked': 677, 'shone': 678, 'shooting': 679, 'shot': 680, 'shoulder': 681, 'shoulders': 682, 'shouted': 683, 'showers': 684, 'showing': 685, 'shuddered': 686, 'sideways': 687, 'sight': 688, 'sightings': 689, 'sign': 690, 'signs': 691, 'silence': 692, 'silent': 693, 'silently': 694, 'silly': 695, 'silver': 696, 'simply': 697, 'since': 698, 'single': 699, 'sinking': 700, 'sipped': 701, 'sir': 702, 'sirius': 703, 'sister': 704, 'sit': 705, 'sitting': 706, 'size': 707, 'sky': 708, 'slammed': 709, 'sleepiness': 710, 'sleeping': 711, 'slept': 712, 'slinking': 713, 'slipped': 714, 'small': 715, 'smile': 716, 'smiling': 717, 'snapped': 718, 'sniff': 719, 'sniffed': 720, 'sobbed': 721, 'somebody': 722, 'somehow': 723, 'someone': 724, 'something': 725, 'son': 726, 'soon': 727, 'sound': 728, 'sounding': 729, 'sparkling': 730, 'spectacles': 731, 'sped': 732, 'spend': 733, 'spent': 734, 'split': 735, 'spoke': 736, 'spot': 737, 'spotted': 738, 'spying': 739, 'square': 740, 'squeaky': 741, 'stand': 742, 'standing': 743, 'stare': 744, 'stared': 745, 'staring': 746, 'stars': 747, 'started': 748, 'staying': 749, 'steadily': 750, 'steering': 751, 'step': 752, 'stepped': 753, 'stern': 754, 'stiff': 755, 'stiffly': 756, 'still': 757, 'stone': 758, 'stood': 759, 'stop': 760, 'stopped': 761, 'story': 762, 'straight': 763, 'strange': 764, 'strangely': 765, 'stranger': 766, 'streaming': 767, 'street': 768, 'streets': 769, 'stretch': 770, 'stroked': 771, 'struck': 772, 'stuff': 773, 'stumbled': 774, 'stunt': 775, 'stupid': 776, 'subject': 777, 'suddenly': 778, 'suggest': 779, 'sunrise': 780, 'suppose': 781, 'supposed': 782, 'sure': 783, 'surely': 784, 'surprised': 785, 'swapping': 786, 'sweet': 787, 'sweets': 788, 'swelled': 789, 'swept': 790, 'swish': 791, 'swooped': 792, 'swooping': 793, 'swung': 794, 'tabby': 795, 'tail': 796, 'take': 797, 'taking': 798, 'tall': 799, 'tangles': 800, 'tantrum': 801, 'tawny': 802, 'tea': 803, 'telephone': 804, 'tell': 805, 'tend': 806, 'ter': 807, 'thank': 808, 'thankful': 809, 'the': 810, 'there': 811, 'these': 812, 'they': 813, 'thin': 814, 'thing': 815, 'things': 816, 'think': 817, 'thinking': 818, 'this': 819, 'though': 820, 'thought': 821, 'three': 822, 'threw': 823, 'throat': 824, 'throwing': 825, 'tidy': 826, 'tie': 827, 'tight': 828, 'time': 829, 'times': 830, 'tin': 831, 'tiny': 832, 'today': 833, 'together': 834, 'told': 835, 'tonight': 836, 'took': 837, 'top': 838, 'toward': 839, 'town': 840, 'traffic': 841, 'trash': 842, 'trembled': 843, 'trick': 844, 'tried': 845, 'true': 846, 'trust': 847, 'trying': 848, 'tuck': 849, 'tucked': 850, 'tuesday': 851, 'tuft': 852, 'turn': 853, 'turned': 854, 'turning': 855, 'twelve': 856, 'twice': 857, 'twinkling': 858, 'twitched': 859, 'two': 860, 'unable': 861, 'unblinkingly': 862, 'uncle': 863, 'under': 864, 'underground': 865, 'underneath': 866, 'understand': 867, 'undursleyish': 868, 'uneasy': 869, 'unsticking': 870, 'unusual': 871, 'unusually': 872, 'unwelcome': 873, 'upset': 874, 'upstairs': 875, 'us': 876, 'use': 877, 'useful': 878, 'usual': 879, 'usually': 880, 'viewers': 881, 'violet': 882, 'voice': 883, 'voldemort': 884, 'waiting': 885, 'wake': 886, 'waking': 887, 'walk': 888, 'walked': 889, 'wall': 890, 'walls': 891, 'want': 892, 'wanted': 893, 'was': 894, 'watch': 895, 'watched': 896, 'watching': 897, 'way': 898, 'we': 899, 'wearing': 900, 'weather': 901, 'weeks': 902, 'weirdos': 903, 'well': 904, 'went': 905, 'wet': 906, 'what': 907, 'whatever': 908, 'wheel': 909, 'when': 910, 'whether': 911, 'while': 912, 'whiskery': 913, 'whisper': 914, 'whispered': 915, 'whisperers': 916, 'whispering': 917, 'who': 918, 'whole': 919, 'wide': 920, 'wife': 921, 'wild': 922, 'window': 923, 'wiping': 924, 'wise': 925, 'without': 926, 'woke': 927, 'woken': 928, 'woman': 929, 'wondered': 930, 'word': 931, 'words': 932, 'world': 933, 'worried': 934, 'worrying': 935, 'would': 936, 'wounded': 937, 'wrestled': 938, 'written': 939, 'wrong': 940, 'yawned': 941, 'years': 942, 'yelled': 943, 'you': 944, 'young': 945}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch loss =  57393.70048310324\n",
      "2 epoch loss =  57318.80950635782\n",
      "3 epoch loss =  57246.31363650881\n",
      "4 epoch loss =  57175.90475678455\n",
      "5 epoch loss =  57107.29793169755\n",
      "6 epoch loss =  57040.228266839375\n",
      "7 epoch loss =  56974.44800981127\n",
      "8 epoch loss =  56909.72390595598\n",
      "9 epoch loss =  56845.83481943902\n",
      "10 epoch loss =  56782.56962804117\n",
      "11 epoch loss =  56719.72539877922\n",
      "12 epoch loss =  56657.10585133976\n",
      "13 epoch loss =  56594.520117415836\n",
      "14 epoch loss =  56531.7818066184\n",
      "\n",
      "Predictions : \n",
      "ron not found in dictionary\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "training_data = preprocessing(corpus)\n",
    "\n",
    "\n",
    "w2v = word2vec()\n",
    "\n",
    "prepare_data_for_training(training_data, w2v)\n",
    "\n",
    "w2v.train(epochs)\n",
    "\n",
    "print()\n",
    "print(\"Predictions : \")\n",
    "print(w2v.predict(\"ron\", 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413c88e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
